services:
  api:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - falkordb
      - ollama
      - redis
    env_file:
      - ./backend/.env
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=granite3.3:8b-largectx
      - FALKOR_HOST=falkordb
      - FALKOR_PORT=6379
      - REDIS_HOST=redis
      - REDIS_PORT=6380
    volumes:
      - ./backend:/app
    working_dir: /app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    working_dir: /app
    command: npm run dev -- --host 0.0.0.0

  falkordb:
    image: falkordb/falkordb:latest
    ports:
      - "6379:6379"
      - "3000:3000"
    volumes:
      - falkor_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped

  ollama:
    build:
      context: ./backend
      dockerfile: ollama.Dockerfile
    # Pull and serve granite3.3:8b on startup
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0:11434
    entrypoint: /app/start-ollama.sh
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./backend/start-ollama.sh:/app/start-ollama.sh
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6380:6380"
    command: redis-server --port 6380
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6380", "ping"]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped

volumes:
  falkor_data:
  ollama_data:
  redis_data: