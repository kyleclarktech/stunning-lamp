services:
  api:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      falkordb:
        condition: service_healthy
    env_file:
      - ./backend/.env
    volumes:
      - ./backend:/app
    working_dir: /app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    working_dir: /app
    command: npm run dev -- --host 0.0.0.0

  falkordb:
    image: falkordb/falkordb:latest
    ports:
      - "6379:6379"
      - "3000:3000"
    volumes:
      - falkor_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped

  # Ollama service (uncomment to enable local AI)
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   restart: unless-stopped
  #   # Uncomment if you have GPU support
  #   # runtime: nvidia
  #   # environment:
  #   #   - NVIDIA_VISIBLE_DEVICES=all

volumes:
  falkor_data:
  # ollama_data: